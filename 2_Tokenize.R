require(quanteda)
require(readtext)
library(broom)
library(dplyr)
library(tm)

#### 5 ngram
startTime <- Sys.time()
#load("cleanfullblog.Rda")
#load("cleanfulltweet.Rda")
load("cleanfullnews.Rda")
mycorpus <- corpus(cleanfullnews)
rm(cleanfullnews)
gc()
mytoken <- tokenize(mycorpus, what = "word", ngrams = 3,remove_symbols = TRUE, remove_numbers = FALSE, remove_punct = FALSE, remove_twitter = TRUE, remove_url = TRUE, remove_separators = TRUE, concatenator = " ")
rm(mycorpus)
gc()
dfmfile <- dfm(mytoken, tolower = FALSE, stem = FALSE, verbose = FALSE)
rm(mytoken)
gc()
save(dfmfile, file= "newsdfm3.Rda")
load("newsdfm6all.Rda")
dfmfile <- dfm_trim(dfmfile, min_count = 4)
df1 <- convert(dfmfile, to = "data.frame") 
rm(dfmfile)
gc()
df_ <- tidy(df1)
rm(df1)
gc()
dffile <- df_ %>%  select(column, max) %>%  arrange(desc(max) )
colnames(dffile)<- c("word","count")
save(dffile,file = 'dfnews3.Rda')
rm(dffile)
gc()


##### 4 ngram
#load("cleanfullblog.Rda")
load("cleanfulltweet.Rda")
#load("cleanfullnews.Rda")
mycorpus <- corpus(cleanfulltwit)
rm(cleanfulltwit)
gc()
mytoken <- tokenize(mycorpus, what = "word", ngrams = 3,remove_symbols = TRUE, remove_numbers = FALSE, remove_punct = FALSE, remove_twitter = TRUE, remove_url = TRUE, remove_separators = TRUE, concatenator = " ")
rm(mycorpus)
gc()
dfmfile <- dfm(mytoken, tolower = FALSE, stem = FALSE, verbose = FALSE)
rm(mytoken)
gc()
dfmfile <- dfm_trim(dfmfile, min_count = 4)
df1 <- convert(dfmfile, to = "data.frame")
rm(dfmfile)
gc()
df_ <- tidy(df1)
rm(df1)
gc()
dffile <- df_ %>%  select(column, max) %>%  arrange(desc(max) )
colnames(dffile)<- c("word","count")
save(dffile,file = 'dftweet3.Rda')
rm(dffile)
gc()



##### 3 ngram
load("cleanfullblog.Rda")
#load("cleanfulltweet.Rda")
#load("cleanfullnews.Rda")
mycorpus <- corpus(cleanfullblog)
rm(cleanfullblog)
gc()
mytoken <- tokenize(mycorpus, what = "word", ngrams = 3,remove_symbols = TRUE, remove_numbers = FALSE, remove_punct = FALSE, remove_twitter = TRUE, remove_url = TRUE, remove_separators = TRUE, concatenator = " ")
rm(mycorpus)
gc()
dfmfile <- dfm(mytoken, tolower = FALSE, stem = FALSE, verbose = FALSE)
rm(mytoken)
gc()
dfmfile <- dfm_trim(dfmfile, min_count = 4)
df1 <- convert(dfmfile, to = "data.frame") #quanteda
rm(dfmfile)
gc()
df_ <- tidy(df1) #broom
rm(df1)
gc()
dffile <- df_ %>%  select(column, max) %>%  arrange(desc(max) )
colnames(dffile)<- c("word","count")
save(dffile,file = 'dfblog3.Rda')
rm(dffile)
gc()



##### 2 ngram
load("cleanfullblog.Rda")
#load("cleanfulltweet.Rda")
#load("cleanfullnews.Rda")
mycorpus <- corpus(cleanfullblog)
rm(cleanfullblog)
gc()
mytoken <- tokenize(mycorpus, what = "word", ngrams = 2,remove_symbols = TRUE, remove_numbers = FALSE, remove_punct = FALSE, remove_twitter = TRUE, remove_url = TRUE, remove_separators = TRUE, concatenator = " ")
rm(mycorpus)
gc()
dfmfile <- dfm(mytoken, tolower = FALSE, stem = FALSE, verbose = FALSE)
rm(mytoken)
gc()
dfmfile <- dfm_trim(dfmfile, min_count = 2)
df1 <- convert(dfmfile, to = "data.frame")
rm(dfmfile)
gc()
df_ <- tidy(df1)
rm(df1)
gc()
dffile <- df_ %>%  select(column, max) %>%  arrange(desc(max) )
colnames(dffile)<- c("word","count")
save(dffile,file = 'dfblog2.Rda')
rm(dffile)
gc()


